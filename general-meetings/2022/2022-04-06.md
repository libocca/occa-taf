# 2022-04-06

Thank you to all who were able to attend for contributing to the discussion!

## Attendees

- Tim Warburton (Virginia Tech)
- Thomas Applencourt (Argonne)
- Stefan Frijters (Shell)
- Noel Chalmers (AMD)
- Umesh Unnikrishnan (Argonne)
- Cedric Andreolli (Intel)
- Amik St-Cyr (Shell)
- Saumil Sudhir Patel (Argonne)
- Kris Rowe (Argonne)


## Follow-up
*Action items from last meeting*

- [x] Create structure on GitHub for contributing to the documentation efforts
- [ ] Find volunteers to lead the efforts in various aspects of user documentation

## Agenda

1. Brief update on documentation efforts.
2. Compilation  
   - User requirements  
     - Large parallel filesystems
     - Deploying precompiled kernels
   - AOT Compilation
   - JIT Compilation

## Notes

### Update on Documentation Efforts

- New issues have been added on GitHub to cover documentation efforts discussed previously
- More volunteers are needed to take on specific areas
- Umesh has volunteered to help with examples
- Cedric and Amik will contribute examples for the iso3DFD kernel

### Compilation

*Overall caching is the common theme/difficulty related to most of the use cases and topics dicussed*

#### User Requirements

1. Large Parallel Filesystems
   - [NekRS](https://github.com/Nek5000/nekRS/blob/master/src/core/platform.cpp)
2. Deploying precompiled kernels
3. Batching many jobs with smaller numbers of nodes (utilizing the same core kernels)

*Currently many user codes need to do a dry-run first to trigger jitting*

#### Ahead-of-time Compilation

- Currently this can be done through the OCCA commandline tool
  - Somewhat cumbersome to use

##### Proposals  

1. Implement a shell script or separate commandline tool, `occacxx`&mdash;similar to `mpicxx`
2. Implement functionality to link kernels into a single module/dll
   - Reduce filesystem stress at runtime, potentially side-step LPFS issues
   - Complements the kernel bundles proposal below
3. Create CMake scripts to integrate `occacxx` into application build systems

- *We will defer this for later discussion*
- *Per Tim's suggestion: a prototype would be useful for understanding difficulty*
- *Benchmark prototype to determine the value added.*
- Kernel caching, hashing likely needs to be revisited first

#### JIT Compilation  

###### Proposals

1. Kernel Bundles&mdash;e.g., see
   - [SYCL 2020 spec](https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:interfaces.bundles)
   - [OpenCL 3.0 spec](https://www.khronos.org/registry/OpenCL/specs/3.0-unified/html/OpenCL_API.html#_program_objects)
2. Support for asynchronous jitting
   - Use [C++ concurrency support library](https://en.cppreference.com/w/cpp/thread)
   - See [this draft PR](https://github.com/libocca/occa/pull/571) for an example.
3. File Abstraction to allow for plugin implementation
   - Default to using [C++17 filesystem library](https://en.cppreference.com/w/cpp/filesystem)
   - Implement an optional plugin for parallel filesytems using MPI-IO
4. API for MPI aware JIT&mdash;*thanks to Stefan K. for suggestion*
   ```
   buildKernelMPI(const string& fileName, const string& kernelName, const json& props, int root, const MPI_Comm& comm)
   ```
   - Goals  
      - Scaleable and lock-free
      - Cache generated binaries to accelerate build process
   - Example Implementation
      - Build on root; fsync all build artifacts to ensure durability
      - Broadcast build files to all compute nodes
      - Node local root writes to `$TMPDIR`, calls fsync
      - Load kernel binary from `$TMPDIR` on all ranks 
    - Caveats
       - Introduces a new&mdash;but optional&mdash;dependency on MPI (during initial OCCA build)
       - Device driver may cache binaries internally (turn off)
       - Using `system` to invoke the compiler is non-portable, however not all backends provide a runtime options (e.g. serial)  

##### Comments

- A prototype and use cases will be developed for kernel bundles for discussion at a future meeting
- General agreement was to work on making OCCA API thread-safe, however users will be responsible for implementing concurrency
- Since we already require C++17 for the SYCL/DPC++ backend, it is reasonable to use the filesystem library
   - Compiler support needs to be investigated and documented
- The idea of a filesystem plugin will be discussed further at the next meeting.
   - Following SOILD principles, it would be better to avoid introducing MPI dependencies into OCCA
   - This can likely be acheived using dependency injection
- MPI jitting will be discussed at the next meeting

## Action Items

- [ ] Document compiler support for C++17
- [ ] Find volunteers to lead the efforts in various aspects of user documentation

## Next Meeting

- [2022-04-20](2022-04-20.md)

## Suggested Future Topics

- MPI Jit
   - How to bubble up information from OCCA to higher level libraries to avoid introducing MPI dependency into OCCA itself
- C++17
- Spack
- OKL Preprocessor, kernel templates
- C preprocessor 
- OpenMP backend
